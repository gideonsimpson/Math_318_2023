---
title: "3/13/2023 Lecture"
output: html_notebook
---

```{r}
library(tidyverse)
library(glmnet) # this ridge regression
```

# Ridge Regression, Continued

Generate data. We have 100 predictors, but only $X_1$ determines $Y$

```{r}
set.seed(100)

# number of observations
n = 100
# total number of predictors
p = 100
# the x1 predictor and y
x1 <- runif(0,1,n=n)
y <- 1 + 2 * x1 + 0.1 * rnorm(n) #true model, \beta_0 = 1, \beta_1 = 2

# fill the other p-1 variables with Gaussian noise by
# first populating a matrix
x_rest <- matrix(0.01 * rnorm(n *(p-1)), n, p-1)
# then we name the columns using colnames and paste
colnames(x_rest) <- paste("x",2:p, sep="")
# now we glue the columns together into a single matrix
data.matrix <- cbind(x1, x_rest, y)

# this reinterprets the matrix as a data frame
data.df <- as_tibble(data.matrix)
```

Extract design matrix:

```{r}
X <- model.matrix(y~., data.df)[,-1] # -1 omits the first column, corresponding to the intercept
```

Run Ridge regression on a whole range of $\lambda$ values, since we do not know which one is "right."

```{r}
low = -4;
high = 2;
lambda.grid <- 10^seq(low, high,length = 100)
ridge.fit <- glmnet(X,y, alpha = 0, lambda = lambda.grid)
```

Run cross validation and visualize:

```{r}
set.seed(200)
cv.ridge <- cv.glmnet(X,y,alpha=0,lambda = lambda.grid)
# visualize using ggplot2
cv.df <- tibble(lambda=cv.ridge$lambda, cvm=cv.ridge$cvm,
cvlo=cv.ridge$cvlo,cvup=cv.ridge$cvup)
cv.plt <- ggplot(cv.df) + geom_point(aes(x=lambda,y=cvm),color="red") +
geom_errorbar(aes(x=lambda, ymin=cvlo, ymax=cvup),alpha = .25) +
geom_vline(xintercept = cv.ridge$lambda.min,linetype = "dashed")+
geom_vline(xintercept = cv.ridge$lambda.1se,linetype = "dashed")+
ggtitle("Ridge Regression Error") +
ylab("Cross Validated MSE") +
scale_x_log10()
print(cv.plt)
```

```{r}
cv.ridge$lambda.min
cv.ridge$lambda.1se
```

```{r}
beta.minimal <- predict(ridge.fit,type="coefficients", s = cv.ridge$lambda.min)
beta.minimal[1:4]
```

# LASSO

Perform variable selection. Change to `alpha = 1` for LASSSO; `alpha=0` for ridge

```{r}
# switch alpha = 1 fro LASSO
lasso.fit <- glmnet(X,y, alpha = 1, lambda = lambda.grid)
```

```{r}
# extract coefficients
beta <-as.matrix(coef(lasso.fit))
beta.df <- as_tibble(beta)
beta.df$coef <- row.names(beta)
beta.df.long <- gather(beta.df,key=case,value,-coef)
beta.df.long$case <- as.integer(gsub("s", "", beta.df.long$case))
beta.df.long$lambda <-lasso.fit$lambda[beta.df.long$case+1]

# plot all coefficients
beta.plt <- ggplot(beta.df.long[beta.df.long$coef!="(Intercept)",], 
                   aes(x=lambda, y=value,color = coef,linetype = coef)) + 
  geom_line() + theme(legend.position="none") +
  scale_x_log10()  + ggtitle("LASSO Coefficients") + ylab("Coefficients")
print(beta.plt)
```

Cross validate

```{r}
set.seed(200)
cv.lasso <- cv.glmnet(X,y,alpha=1,lambda = lambda.grid) # alpha = 1 for LASSO
# visualize using ggplot2
cv.df <- tibble(lambda=cv.lasso$lambda, cvm=cv.lasso$cvm,
cvlo=cv.lasso$cvlo,cvup=cv.lasso$cvup)
cv.plt <- ggplot(cv.df) + geom_point(aes(x=lambda,y=cvm),color="red") +
geom_errorbar(aes(x=lambda, ymin=cvlo, ymax=cvup),alpha = .25) +
geom_vline(xintercept = cv.lasso$lambda.min,linetype = "dashed")+
geom_vline(xintercept = cv.lasso$lambda.1se,linetype = "dashed")+
ggtitle("LASSO Error") +
ylab("Cross Validated MSE") +
scale_x_log10()
print(cv.plt)
```

```{r}
cv.lasso$lambda.min
beta.minimal <- predict(lasso.fit,type="coefficients", s = cv.lasso$lambda.min)
beta.minimal[1:10]
nnzero(beta.minimal) # count number of nonzero beta's 
```
