---
title: "2/20/2023 Lecture"
output: html_notebook
---

```{r}
library(tidyverse)
library(ISLR2)
library(MASS) # for LDA and QDA regressions
```

# LDA Example

Generate the mixture model data:

```{r}
set.seed(100) # set seed for reproducibility
# defines and the standard devs of the two Gaussians
mu <- c(-1, 2)
sigma <- c(1., 1.)
# prior probabilities
prior <- c(0.3, 0.7)

# number of total samples we draw from the mixture model
n = 10^4

# first sample the class varaible n times
class<- sample(1:2,n,replace = TRUE, prob = prior)
# convert to categorical variable
class <- as.factor(class)
# sample the x values (what we measure in practice)
# preallocates the array
x <- numeric(n)
for(j in 1:n){
  # sample Gaussian 1 or Gaussian 2 as appropriate
  x[j] <- rnorm(1,mean = mu[class[j]], sd = sigma[class[j]])
}
# store as a tibble
mixture.df <- tibble("class"=class, "x"=x)

```

```{r}
mixture.df
```

```{r}
lda.fit <- lda(class~x, data = mixture.df) # fits the LDA classifier, given the data, and the model class ~ x
```

Check whats contained in `lda`

```{r}
lda.fit
```

```{r}
# generates the predictions on the trained classifier
pred.train <- predict(lda.fit)
# construct a data frame for plotting
lda.df <- tibble(x=mixture.df$x, true = mixture.df$class, # truth
                     pred=pred.train$class, p=pred.train$posterior[,1], # predicted and probability
                     correct = (mixture.df$class==pred.train$class)) # correct or not
plot.lda <- ggplot(data = lda.df, aes(shape=pred, color=correct)) +
  geom_point(aes(x=x,y=p))+ggtitle("LDA Classifier")
print(plot.lda)

```

Getting $\hat{\sigma}$:

```{r}
sigma2 <-  0
for(c in 1:2){
  x <- filter(mixture.df,class==c)$x
  sigma2 <- sigma2 + sum(x^2 - mean(x)^2)
  }
sigma2 <- sigma2/(nrow(mixture.df)-2)
sigma2
```

Compare with logistic classifier

```{r}
logistic.fit <- glm(class~x,data=mixture.df, family = binomial)
logistic.prob <- as_tibble(predict(logistic.fit, type = "response"))
logistic.pred<- rep(1, nrow(mixture.df))
logistic.pred[logistic.prob>0.5] <-2
logistic.df <- tibble(x=mixture.df$x, true = mixture.df$class,
                   pred=as.factor(logistic.pred), p=pred.train$posterior[,1],
                   correct = (mixture.df$class==logistic.pred))
plot.logistic <- ggplot(data = logistic.df, aes(shape=pred, color=correct)) +
  geom_point(aes(x=x,y=p))+ggtitle("Logistic Classifier")
print(plot.logistic)

```

Compare performance:

```{r}
pred.train <- predict(lda.fit)
mean(pred.train$class != mixture.df$class)
table(pred.train$class, mixture.df$class)

```

```{r}
logistic.prob <- predict(logistic.fit, type = "response")
logistic.pred<- rep(1, nrow(mixture.df))
logistic.pred[logistic.prob>0.5] <-2
mean(logistic.pred != mixture.df$class)

```
