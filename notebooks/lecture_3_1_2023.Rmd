---
title: "3/1/2023 Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(ISLR2)
library(MASS) # for LDA and QDA regressions
library(FNN)
library(boot)
```

# Variability

Try different degree polynomial regressions for data generated by

$$
X \sim U(0,5), \quad Y =e^{X}(1+\epsilon), \quad \epsilon \sim N(0, 1/4)
$$
\
Note that this is NOT of type $Y = f(X) + \epsilon$. We say there is *multiplicative* noise in this problem.

Regress this with different polynomials of different degree,

$$
\hat{f}(X) = \beta_0 + \beta_1 X + \ldots + \beta_p X^p
$$

```{r}
set.seed(100)

# for each p generate training sets of size n and testing sets of size n
# test/train sample size
n = 10^2
# repeat each experiment (each p) m times
# number of independent runs
m = 10^2

# highest degree polynomial we will try
maxp = 10

# preallocate an array with zeros
test.MSE.vals <- rep(0,maxp)
train.MSE.vals <- rep(0,maxp)

# generate the training set
x <- runif(n,0,5)
y <- exp(x)*(1 + .5 * rnorm(n))
train.df <- tibble("x"=x, "y"=y)

# loop over data sets and polynomial fits
for (j in seq(m)){
# generate testing data
  x <- runif(n,0,5)
  y <- exp(x)*(1 + .5 * rnorm(n))
  test.df <- tibble("x"=x, "y"=y)
  # try all the polynomial fits on the testing data
  for(p in seq(maxp)){
    lm.fit <- lm(y~poly(x,p), data=train.df)
    train.MSE.vals[p] =train.MSE.vals[p] +
      mean(lm.fit$residuals^2)/m
    test.MSE.vals[p] = test.MSE.vals[p] +
      mean((test.df$y - predict(lm.fit,test.df))^2)/m
  }
}
train.mse.df <- tibble(p = seq(maxp), MSE=train.MSE.vals)
test.mse.df <- tibble(p = seq(maxp), MSE=test.MSE.vals)
train.mse.df$Type <- "Train"
test.mse.df$Type <- "Test"
mse.df <- rbind(train.mse.df, test.mse.df)
mse.df$p <- as.integer(mse.df$p)

gplt <- ggplot(mse.df,mapping = aes(x=p, y=MSE, color=Type)) +
  geom_line(lwd=2) +
  labs(x ="Degree Polynomial", y = "MSE", title="MSE as a Function of Fit",
       subtitle=sprintf("%d independent testing sets of size %d", m,n)) +
  theme_classic()
print(gplt)

```

Check the regressions visually:

```{r}

fit.plt <- ggplot(train.df, aes(x,y)) +
  geom_point() +
  ggtitle("Training Set") +
  geom_smooth(method = lm, 
              formula = y~x,
              se=TRUE,aes(color="p=1")) +
  geom_smooth(method = lm, 
              formula = y~poly(x,maxp),
              se=TRUE,aes(color="p=10")) + 
  geom_smooth(method = lm, 
              formula = y~poly(x,3),
              se=TRUE,aes(color="p=3")) + 
  stat_function(fun=exp, lwd=2,linetype = 2) + 
  ggtitle("Comparison of Training Data with Regressions") +
  scale_color_manual(name="Regressions", values=c("blue", "red", "green","black" ))+
  theme_classic()
print(fit.plt)

```

## Variability due to training set size

```{r}

# reset seed
set.seed(100)

# training set size values
n_train <- c(5, 10,20, 40, 80)

# validation (testing) set is fixed
n_test <- 10^4

# set true coefficients
beta0_true = 2.
beta1_true = -1.5
# set noise parameters
eps = 1.

# construct validation set, fixed over the entire computation
x_test <-  runif(n_test,0,15)
y_test <-  beta0_true + beta1_true*x_test + eps * rnorm(length(x_test))
test.df <- data.frame("x"= x_test, "y"=y_test)

# number of samples per training set size
n_samples = 100

# these arrays begin as null but will be populated
MSE_vals <- NULL
n_vals <- NULL

# loop over each training set size n_samples times construct training
# data, train, compute the MSE, and record
for(j in 1:length(n_train)){
  for(i in 1:n_samples){
    n <- n_train[j]
    x <- runif(n,0,15)
    y <-  beta0_true + beta1_true*x + eps * rnorm(length(x))
    train.df <- data.frame("x"=x, "y"=y)
    lm.fit <- lm(y~x, data=train.df)

    # record the n and the MSE value at which this was computed
    n_vals <- append(n_vals,n)
    MSE_vals <- append(MSE_vals,mean((predict(lm.fit,test.df) -test.df$y)^2))
  }
}
# record as a data frame and post process
MSE.df = data.frame("n"=n_vals, "MSE"=MSE_vals)
MSE.df$n <- as.factor(MSE.df$n)

MSE.plt <- ggplot(MSE.df, aes(x=n, y=MSE_vals)) +geom_boxplot() +
  ggtitle("Mean Squared Error of a Linear Model",
          subtitle = sprintf("%d Test Sets of Size %d", n_samples, n_test)) +
  scale_y_log10()+
  labs(x="Training Set Size", y = "MSE") + theme_classic()
print(MSE.plt)

```

## Variability over different testing sets

```{r}
set.seed(100)

# test/train sample size
n = 10^3
# number of splits
m = 5

maxp = 25

# preallocate an array with zeros
test.MSE.vals <- NULL
trial.vals <- NULL
p.vals <- NULL

# generate the entire data set
x <- runif(n,0,5)
y <- exp(x)*(1 + .5 * rnorm(n))
data.df <- tibble("x"=x, "y"=y)

# loop over data sets and polynomial fits
for (j in seq(m)){
  idx.train <- sample(seq(n), size = n/2)
  train.df <- data.df[idx.train,]
  test.df <- data.df[-idx.train,]
  # try all the polynomial fits on the testing data
  MSE =0
  for(p in seq(maxp)){
    lm.fit <- lm(y~poly(x,p), data=train.df)
    test.MSE.vals <- append(test.MSE.vals,mean((test.df$y - predict(lm.fit,test.df))^2))
    p.vals <- append(p.vals, p)
    trial.vals <- append(trial.vals, j)
  }
}

mse.df <- tibble(p=p.vals, MSE=test.MSE.vals, Trial=trial.vals)
mse.df$p <- as.integer(mse.df$p)
mse.df$Trial <- as.factor(mse.df$Trial)

val.plt <- ggplot(mse.df,mapping = aes(x=p, y=MSE, color=Trial)) +
  geom_line(lwd=2) +
  labs(x ="Degree of Polynomial", y = "MSE", title="50% Train-Test Data Split") +
  theme_classic()+scale_y_log10()
print(val.plt)

```
